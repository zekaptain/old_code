{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zacharyelkins/anaconda/lib/python2.7/site-packages/bs4/__init__.py:182: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/Users/zacharyelkins/anaconda/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.happierabroad.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import numpy\n",
    "from gensim.models import word2vec\n",
    "\n",
    "def clean_sentence( raw ):\n",
    "    bs = BeautifulSoup(raw)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\",\" \",bs.get_text())\n",
    "    lower_case = letters_only.lower()\n",
    "    words = lower_case.split()\n",
    "    return words\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def review_to_sentences( review, tokenizer):\n",
    "    #didn’t seem to work without it, thanks StackOverflow\n",
    "    review = review.decode('utf-8')\n",
    "    #strip out whitespace at beginning and end\n",
    "    review = review.strip()\n",
    "    raw_sentences = tokenizer.tokenize(review)\n",
    "    sentences_list = []\n",
    "    for sentence in raw_sentences:\n",
    "        if len(sentence) > 0: #skip it if the sentence is empty\n",
    "            cl_sent = clean_sentence(sentence)\n",
    "            sentences_list.append(cl_sent)\n",
    "    return sentences_list\n",
    "\n",
    "data = pandas.read_csv(\"imdb_reviews.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "#somehow need to make a sentences_for_all_reviews list\n",
    "sentences_for_all_reviews = []\n",
    "\n",
    "#clean_sentence(data)\n",
    "for review in data['review']:\n",
    "    sentences = review_to_sentences(review,tokenizer)\n",
    "    sentences_for_all_reviews += sentences\n",
    "\n",
    "\n",
    "num_attributes = 300 # Word vector dimensionality\n",
    "min_word_count = 40 # Minimum word frequency\n",
    "num_workers = 4 # Number of threads to run in parallel\n",
    "context = 10 # Context window size\n",
    "downsampling = 1e-3 # Downsample setting for frequent words\n",
    "# Initialize and train the model (this will take some time)\n",
    "model = word2vec.Word2Vec(sentences_for_all_reviews,workers=num_workers, size=num_attributes,min_count = min_word_count,window = context, sample = downsampling)\n",
    "#saves memory if you’re done training it\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "model.vocab\n",
    "'chicago' in model.vocab\n",
    "'iowa' in model.vocab\n",
    "model.similarity('england','france')\n",
    "model.similarity('england','paris')\n",
    "model.most_similar('king')\n",
    "model.most_similar('awful')\n",
    "model.doesnt_match(\"man woman child kitchen\".split())\n",
    "model.doesnt_match(\"france england germany berlin\".split())\n",
    "model['king']\n",
    "model['queen']\n",
    "model['man']\n",
    "model['woman']\n",
    "(model['king'] - model['man'] + model['woman'])\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "\n",
    "def makeAttributeVec(words, model, num_attributes):\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    attributeVec = numpy.zeros((num_attributes,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    # Loop over each word in the review and, if it is in the model’s\n",
    "    # vocabulary, add its attribute vector to the total\n",
    "    for word in words:\n",
    "        if word in model.vocab:\n",
    "            nwords = nwords + 1.\n",
    "            attributeVec = numpy.add(attributeVec,model[word])\n",
    "            \n",
    "    # Divide the result by the number of words to get the average\n",
    "    attributeVec = numpy.divide(attributeVec,nwords)\n",
    "    return attributeVec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
