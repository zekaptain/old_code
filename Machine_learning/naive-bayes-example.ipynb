{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "\n",
    "iris_data = data = pandas.read_csv('irisData.csv')\n",
    "\n",
    "(iris_train,iris_test) = cv.train_test_split(iris_data,test_size=0.2)\n",
    "\n",
    "perc_alg = lm.Perceptron()\n",
    "predictors = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\"]\n",
    "perc_alg.fit(iris_train[predictors],iris_train[\"species\"])\n",
    "\n",
    "iris_test_preds = perc_alg.predict(iris_test[predictors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150 points : 6\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"% (iris.data.shape[0],(iris.target != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.725\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = pandas.read_csv(\"imdb_reviews.tsv\", delimiter=\"\\t\")\n",
    "#print out the first review\n",
    "#print data[\"review\"][0]\n",
    "\n",
    "cleaned_review = []\n",
    "\n",
    "def cleanReview(review):\n",
    "    rev_soup = BeautifulSoup(review)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\",\" \",rev_soup.get_text())\n",
    "    #print letters_only\n",
    "\n",
    "    lower_case = letters_only.lower()\n",
    "    words = lower_case.split()\n",
    "    #print words\n",
    "\n",
    "    #print out the nltk stop words list\n",
    "    #print stopwords.words(\"english\")\n",
    "\n",
    "    clean_text = \" \".join(words)\n",
    "    cleaned_review.append(clean_text) \n",
    "\n",
    "#rev_soup = BeautifulSoup(data[\"review\"][0])\n",
    "\n",
    "#save first 1000 reviews to this \n",
    "first1000 = data[\"review\"][0:1000]\n",
    "for x in first1000:    \n",
    "    cleanReview(x)\n",
    "\n",
    "#pass all reviews into cleanReview array\n",
    "'''allrev = data[\"review\"][0:]\n",
    "for x in allrev:\n",
    "    cleanReview(x)'''\n",
    "\n",
    "#print cleaned_review\n",
    "\n",
    "#Bag of Words with 5000 most common words\n",
    "vectorizer = CountVectorizer(analyzer='word',max_features = 5000)\n",
    "word_columns = vectorizer.fit_transform(cleaned_review)\n",
    "#convert to numpy array so we can feed it\n",
    "#into learning algorithm\n",
    "word_columns = word_columns.toarray()\n",
    "#print vectorizer.get_feature_names()\n",
    "#print word_columns\n",
    "\n",
    "(train_data, test_data, train_target, test_target) = cv.train_test_split(word_columns, data[\"sentiment\"][0:1000],test_size = 0.2)\n",
    "'''\n",
    "###### MNB \n",
    "mnb = MultinomialNB() \n",
    "mnb.fit(train_data,train_target)\n",
    "preds = mnb.predict(test_data)\n",
    "print accuracy_score(preds,test_target)\n",
    "'''\n",
    "###### PCA before SVM\n",
    "pca = PCA(n_components=100,whiten=True)\n",
    "pca.fit(train_data)\n",
    "transformed_train_data = pca.transform(train_data)\n",
    "transformed_test_data = pca.transform(test_data)\n",
    "\n",
    "######SVM on review data, see how it compares to mnb\n",
    "mnb = SVC() \n",
    "mnb.fit(transformed_train_data,train_target)\n",
    "preds = mnb.predict(transformed_test_data)\n",
    "print accuracy_score(preds,test_target)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
